{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1909bc5-46ba-469e-bdd6-8dd30ac88309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Selenium-related imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f000d1-cddc-4d03-a374-4964363ba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8cd4f4-0130-468e-b52d-ece0079d480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selenium_manual_login():\n",
    "    \"\"\"\n",
    "    Opens a visible browser window and navigates to the Seeking Alpha login page.\n",
    "    Waits for the user to manually log in and then presses ENTER in the notebook to continue.\n",
    "    Returns the session cookies for use with requests.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    # For manual login, do not run headless so you can see and interact with the page.\n",
    "    # chrome_options.add_argument(\"--headless\")  <-- Don't use headless mode.\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/105.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    \n",
    "    # Use webdriver-manager to automatically install and manage ChromeDriver.\n",
    "    chrome_driver_path = \"F:/OneDrive - KAIST/Labs/N22_Prof_Ryonhee/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "    service = Service(chrome_driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Navigating to Seeking Alpha login page...\")\n",
    "        driver.get(\"https://seekingalpha.com/account/login\")\n",
    "        \n",
    "        # Instruct the user to log in manually.\n",
    "        logging.info(\"Please log in manually in the opened browser window. Once you have logged in and the page has fully loaded, press ENTER here to continue...\")\n",
    "        input(\"Press ENTER after logging in...\")\n",
    "        \n",
    "        # Optionally, wait for a login indicator (e.g., user profile element) to confirm login.\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@class, 'user-profile')]\")))\n",
    "            logging.info(\"Login success indicator detected.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Login indicator not detected. Ensure you have successfully logged in.\")\n",
    "        \n",
    "        # Extract cookies from the current session.\n",
    "        cookies = driver.get_cookies()\n",
    "        return cookies\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during manual login: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e08bee-deb3-41d1-bcae-465e8ca0d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cookies_for_requests(cookies_list):\n",
    "    \"\"\"\n",
    "    Converts Selenium cookies (a list of dicts) into a dictionary for use with `requests`.\n",
    "    \"\"\"\n",
    "    cookies_dict = {}\n",
    "    for cookie in cookies_list:\n",
    "        cookies_dict[cookie[\"name\"]] = cookie[\"value\"]\n",
    "    return cookies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd18d58-1ba4-43e3-b36b-89219a7a3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Access Section ---\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/105.0.0.0 Safari/537.36\"),\n",
    "    \"Referer\": \"https://seekingalpha.com/\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d710a5-2b35-4561-9f53-016a3b50a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_listing_page(page_number, cookies):\n",
    "    \"\"\"\n",
    "    Fetches the listing page of earnings call transcripts using the authenticated session cookies.\n",
    "    \"\"\"\n",
    "    base_url = \"https://seekingalpha.com/api/v3/articles\"\n",
    "    params = {\n",
    "        \"filter[category]\": \"earnings::earnings-call-transcripts\",\n",
    "        \"filter[since]\": \"0\",\n",
    "        \"filter[until]\": \"0\",\n",
    "        \"include\": \"author,primaryTickers,secondaryTickers\",\n",
    "        \"isMounting\": \"true\",\n",
    "        \"page[size]\": \"50\",\n",
    "        \"page[number]\": str(page_number)\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            base_url,\n",
    "            headers=HEADERS,\n",
    "            params=params,\n",
    "            cookies=cookies,\n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Listing page {page_number} fetched successfully.\")\n",
    "            return response.json()\n",
    "        else:\n",
    "            logging.error(f\"Error fetching listing page {page_number}: HTTP {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception fetching listing page {page_number}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41d60ed-f536-4658-bcab-f76918c2e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_details(article_id, cookies):\n",
    "    \"\"\"\n",
    "    Fetches the details of a specific article (earnings call transcript) using the article ID.\n",
    "    \"\"\"\n",
    "    url = f\"https://seekingalpha.com/api/v3/articles/{article_id}\"\n",
    "    params = {\n",
    "        \"include\": (\"author,primaryTickers,secondaryTickers,otherTags,\"\n",
    "                    \"presentations,presentations.slides,author.authorResearch,\"\n",
    "                    \"author.userBioTags,co_authors,promotedService,sentiments\")\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=HEADERS,\n",
    "            params=params,\n",
    "            cookies=cookies,\n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Article {article_id} details fetched successfully.\")\n",
    "            return response.json()\n",
    "        else:\n",
    "            logging.error(f\"Error fetching details for article {article_id}: HTTP {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception fetching details for article {article_id}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc522d0a-8869-43a1-b2e9-0448497e3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_data(article_json):\n",
    "    \"\"\"\n",
    "    Extracts the transcript text (by converting HTML to plain text), publication details,\n",
    "    and a company/ticker symbol from the article's JSON data.\n",
    "    \"\"\"\n",
    "    attributes = article_json.get(\"data\", {}).get(\"attributes\", {})\n",
    "    transcript_html = attributes.get(\"body\", \"\")\n",
    "    published_at = attributes.get(\"published_at\")\n",
    "\n",
    "    # Determine publication year and quarter using the published_at timestamp.\n",
    "    if published_at:\n",
    "        try:\n",
    "            pub_date = datetime.fromisoformat(published_at.rstrip(\"Z\"))\n",
    "            year = pub_date.year\n",
    "            month = pub_date.month\n",
    "            if month <= 3:\n",
    "                quarter = \"Q1\"\n",
    "            elif month <= 6:\n",
    "                quarter = \"Q2\"\n",
    "            elif month <= 9:\n",
    "                quarter = \"Q3\"\n",
    "            else:\n",
    "                quarter = \"Q4\"\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error parsing published_at date: {e}\")\n",
    "            year, quarter = \"Unknown\", \"Unknown\"\n",
    "    else:\n",
    "        year, quarter = \"Unknown\", \"Unknown\"\n",
    "\n",
    "    # Use the primary ticker as a proxy for company name.\n",
    "    company = \"Unknown_Company\"\n",
    "    for entry in article_json.get(\"included\", []):\n",
    "        if entry.get(\"type\") == \"primaryTickers\":\n",
    "            attributes_ticker = entry.get(\"attributes\", {})\n",
    "            company = attributes_ticker.get(\"symbol\", \"Unknown_Company\")\n",
    "            break\n",
    "\n",
    "    # Use BeautifulSoup to strip HTML tags.\n",
    "    soup = BeautifulSoup(transcript_html, \"html.parser\")\n",
    "    transcript_text = soup.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    return {\n",
    "        \"company\": company,\n",
    "        \"year\": year,\n",
    "        \"quarter\": quarter,\n",
    "        \"transcript\": transcript_text,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2905e999-e115-4ba6-b1ad-5a46e83e8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript(data, base_dir=\"Transcripts\"):\n",
    "    \"\"\"\n",
    "    Saves the transcript into a file under a directory structure:\n",
    "    Transcripts/<Company>/<Year>/<Quarter>.txt\n",
    "    \"\"\"\n",
    "    company_dir = data[\"company\"].replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    directory = os.path.join(base_dir, company_dir, str(data[\"year\"]))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = os.path.join(directory, f\"{data['quarter']}.txt\")\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data[\"transcript\"])\n",
    "        logging.info(f\"Saved transcript: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcript to {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccc207-06f6-447f-a985-21bf1368c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 18:12:34,818 - INFO - Starting manual login process...\n",
      "2025-01-15 18:12:36,257 - INFO - Navigating to Seeking Alpha login page...\n",
      "2025-01-15 18:12:38,045 - INFO - Please log in manually in the opened browser window. Once you have logged in and the page has fully loaded, press ENTER here to continue...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting manual login process...\")\n",
    "cookies = selenium_manual_login()\n",
    "if not cookies:\n",
    "    logging.error(\"Login failed. Exiting.\")\n",
    "else:\n",
    "    # Convert cookies for use with requests.\n",
    "    session_cookies = convert_cookies_for_requests(cookies)\n",
    "    \n",
    "    # Loop through listing pages to fetch and save earnings call transcripts.\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        logging.info(f\"Fetching listing page {page_number}...\")\n",
    "        listing_json = fetch_listing_page(page_number, session_cookies)\n",
    "        if not listing_json:\n",
    "            logging.info(\"No data returned for listing page; stopping.\")\n",
    "            break\n",
    "\n",
    "        articles = listing_json.get(\"data\", [])\n",
    "        if not articles:\n",
    "            logging.info(\"No more articles found; reached the end of listings.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            article_id = article.get(\"id\")\n",
    "            if not article_id:\n",
    "                continue\n",
    "            details_json = fetch_article_details(article_id, session_cookies)\n",
    "            if not details_json:\n",
    "                continue\n",
    "\n",
    "            data = parse_article_data(details_json)\n",
    "            if data[\"transcript\"]:\n",
    "                save_transcript(data)\n",
    "            else:\n",
    "                logging.warning(f\"No transcript found for article {article_id}\")\n",
    "            \n",
    "            time.sleep(1)  # Be polite to the server\n",
    "        \n",
    "        page_number += 1\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc26845-6fba-487b-9f04-7ab1a87fe273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d975c5-d318-4dc7-b49b-9fca25979009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
